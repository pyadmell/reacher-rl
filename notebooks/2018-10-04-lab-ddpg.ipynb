{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "### Test 1 - DDPG model\n",
    "\n",
    "<sub>Uir√° Caiado. October 07, 2018<sub>\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "_In this notebook, I will use the Unity ML-Agents environment to train a DDPG model for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What we are going to test\n",
    "\n",
    "We begin by checking if the necessary packages are presented. If the code cell below returns an error, please check if you have all the packages required in the README of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.6 64bit [MSC v.1900 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "7.0.1"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.17134 SP0"
        },
        {
         "module": "numpy",
         "version": "1.15.2"
        },
        {
         "module": "unityagents",
         "version": "0.4.0"
        },
        {
         "module": "torch",
         "version": "0.4.1"
        },
        {
         "module": "matplotlib",
         "version": "3.0.0"
        },
        {
         "module": "pandas",
         "version": "0.23.4"
        },
        {
         "module": "gym",
         "version": "0.10.8"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [MSC v.1900 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Windows 10 10.0.17134 SP0</td></tr><tr><td>numpy</td><td>1.15.2</td></tr><tr><td>unityagents</td><td>0.4.0</td></tr><tr><td>torch</td><td>0.4.1</td></tr><tr><td>matplotlib</td><td>3.0.0</td></tr><tr><td>pandas</td><td>0.23.4</td></tr><tr><td>gym</td><td>0.10.8</td></tr><tr><td colspan='2'>Sun Oct 07 01:28:47 2018 Hora oficial do Brasil</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.6 64bit [MSC v.1900 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 7.0.1 \\\\ \\hline\n",
       "OS & Windows 10 10.0.17134 SP0 \\\\ \\hline\n",
       "numpy & 1.15.2 \\\\ \\hline\n",
       "unityagents & 0.4.0 \\\\ \\hline\n",
       "torch & 0.4.1 \\\\ \\hline\n",
       "matplotlib & 3.0.0 \\\\ \\hline\n",
       "pandas & 0.23.4 \\\\ \\hline\n",
       "gym & 0.10.8 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sun Oct 07 01:28:47 2018 Hora oficial do Brasil} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.6 64bit [MSC v.1900 64 bit (AMD64)]\n",
       "IPython 7.0.1\n",
       "OS Windows 10 10.0.17134 SP0\n",
       "numpy 1.15.2\n",
       "unityagents 0.4.0\n",
       "torch 0.4.1\n",
       "matplotlib 3.0.0\n",
       "pandas 0.23.4\n",
       "gym 0.10.8\n",
       "Sun Oct 07 01:28:47 2018 Hora oficial do Brasil"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, unityagents, torch, matplotlib, pandas, gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define some meta variables to use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fig_prefix = 'figures/2018-10-07-'\n",
    "data_prefix = '../data/2018-10-07-'\n",
    "s_currentpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's import some of the necessary packages for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('/notebooks/style/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('/notebooks/style/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('/notebooks/style/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('/notebooks/style/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<style>\n",
       "    table {\n",
       "        overflow:hidden;\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        font-size: 12px;\n",
       "        margin: 10px;\n",
       "        /*width: 480px;*/\n",
       "        text-align: left;\n",
       "        border-collapse: collapse;\n",
       "        border: 1px solid #d3d3d3;\n",
       "        -moz-border-radius:5px; FF1+;\n",
       "        -webkit-border-radius:5px; Saf3-4;\n",
       "        border-radius:5px;\n",
       "        -moz-box-shadow: 0 0 4px rgba(0, 0, 0, 0.01);\n",
       "    }\n",
       "    th\n",
       "    {\n",
       "        padding: 12px 17px 12px 17px;\n",
       "        font-weight: normal;\n",
       "        font-size: 14px;\n",
       "        border-bottom: 1px dashed #69c;\n",
       "    }\n",
       "\n",
       "    td\n",
       "    {\n",
       "        padding: 7px 17px 7px 17px;\n",
       "\n",
       "    }\n",
       "\n",
       "    tbody tr:hover th\n",
       "    {\n",
       "\n",
       "        background:  #E9E9E9;\n",
       "    }\n",
       "\n",
       "    tbody tr:hover td\n",
       "    {\n",
       "\n",
       "        background:  #E9E9E9;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")  # include the root directory as the main\n",
    "import eda\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment used for this project is the Reacher environment, from [Unity](https://youtu.be/heVMs3t9qSk). Bellow, we are going to start this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='C:/GIT/reacher-rl/Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ReacherBrain']\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "print(env.brain_names)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Let's check some information about the environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5a204aa478ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reset the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# number of agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Python API to control the agent and receive feedback from the environment.The idea is to make the agent use its experience to gradually choose better actions when interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.14699999671429395\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "for i in range(1000):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from drlnd.dqn_agent import DQNAgent, DDQNAgent, DDQNPREAgent\n",
    "\n",
    "n_episodes = 2000\n",
    "eps_start = 1.\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "max_t = 1000\n",
    "s_model = 'dqn'\n",
    "\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_std = []                    # List containing the std dev of the last 100 episodes\n",
    "scores_avg = []                    # List containing the mean of the last 100 episodes\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "eps = eps_start                    # initialize epsilon\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    for t in range(max_t):\n",
    "        # action = np.random.randint(action_size)        # select an action\n",
    "        action = agent.act(state, eps)\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    scores_std.append(np.std(scores_window)) # save most recent std dev\n",
    "    scores_avg.append(np.mean(scores_window)) # save most recent std dev\n",
    "    eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=13.0:\n",
    "        s_msg = '\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\n",
    "        print(s_msg.format(i_episode, np.mean(scores_window)))\n",
    "        torch.save(agent.qnet.state_dict(), '%scheckpoint_%s.pth' % (data_prefix, s_model))\n",
    "        break\n",
    "        \n",
    "# save data to use latter\n",
    "d_data = {'episodes': i_episode,\n",
    "          'scores': scores,\n",
    "          'scores_std': scores_std,\n",
    "          'scores_avg': scores_avg,\n",
    "          'scores_window': scores_window}\n",
    "pickle.dump(d_data, open('%ssim-data-%s.data' % (data_prefix, s_model), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent using the DDPG agent was able to solve the Reacher environment in X episodes of 1000 steps, each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "d_data = pickle.load(open('../data/2018-08-24-sim-data-dqn.data', 'rb'))\n",
    "s_msg = 'Environment solved in {:d} episodes!\\tAverage Score: {:.2f} +- {:.2f}'\n",
    "print(s_msg.format(d_data['episodes'], np.mean(d_data['scores_window']), np.std(d_data['scores_window'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the rewards per episode. In the right panel, we will plot the rolling average score over 100 episodes $\\pm$ its standard deviation, as well as the goal of this project (13+ on average over the last 100 episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep Q-learning agent was able to solve the environment in 529 episodes. In the next experiment I will implement and test the Double Deep Q-learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
